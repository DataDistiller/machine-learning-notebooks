{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import keras\n",
    "import sklearn\n",
    "import gensim\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.layers import Embedding, LSTM\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# from word_movers_knn import WordMoversKNN\n",
    "\n",
    "# size of the word embeddings\n",
    "embeddings_dim = 300\n",
    "\n",
    "# maximum number of words to consider in the representations\n",
    "max_features = 30000\n",
    "\n",
    "# maximum length of a sentence\n",
    "max_sent_len = 50\n",
    "\n",
    "# percentage of the data used for model training\n",
    "percent = 0.75\n",
    "\n",
    "# number of classes\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading pre-trained word embeddings...\n"
     ]
    }
   ],
   "source": [
    "resources_base_path = \"/Users/dsbatista/resources/\"\n",
    "\n",
    "print (\"Reading pre-trained word embeddings...\")\n",
    "embeddings = KeyedVectors.load_word2vec_format(\n",
    "    resources_base_path+\"GoogleNews-vectors-negative300.bin.gz\",\n",
    "    binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text data for classification and building representations ...\n"
     ]
    }
   ],
   "source": [
    "# dataset description:\n",
    "# - sentences labelled with positive or negative sentiment, extracted from reviews of products, \n",
    "#   movies, and restaurants\n",
    "# - download from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "\n",
    "data_base_path = \"sentiment_labelled_sentences/\"\n",
    "\n",
    "print(\"Reading text data for classification and building representations ...\")\n",
    "amazon_data = [row for row in csv.reader(open(data_base_path+\"amazon_cells_labelled.txt\"), delimiter='\\t')]\n",
    "random.shuffle(amazon_data)\n",
    "\n",
    "imdb_data = [row for row in csv.reader(open(data_base_path+\"imdb_labelled.txt\"), delimiter='\\t')]\n",
    "random.shuffle(imdb_data)\n",
    "\n",
    "yelp_data = [row for row in csv.reader(open(data_base_path+\"yelp_labelled.txt\"), delimiter='\\t')]\n",
    "random.shuffle(yelp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_data:  1000\n",
      "imdb_data  :  748\n",
      "yelp_data  :  1000\n"
     ]
    }
   ],
   "source": [
    "print(\"amazon_data: \", len(amazon_data))\n",
    "print(\"imdb_data  : \", len(imdb_data))\n",
    "print(\"yelp_data  : \", len(yelp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = amazon_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creat training/testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = int(len(data) * percent)\n",
    "\n",
    "train_texts = [txt.lower().strip() for (txt,label) in data[0:train_size]]\n",
    "test_texts = [txt.lower().strip() for (txt,label) in data[train_size:-1]]\n",
    "\n",
    "train_labels = [label for (txt, label) in data[0:train_size]]\n",
    "test_labels = [label for (txt, label) in data[train_size:-1]]\n",
    "\n",
    "num_classes = len(set(train_labels + test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation: tokenization and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features, lower=True, split=\" \")\n",
    "tokenizer.fit_on_texts(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sequences = sequence.pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=max_sent_len)\n",
    "test_sequences = sequence.pad_sequences(tokenizer.texts_to_sequences(test_texts), maxlen=max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_matrix = tokenizer.texts_to_matrix(train_texts)\n",
    "test_matrix = tokenizer.texts_to_matrix(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes that are considered in the problem : array(['0', '1'],\n",
      "      dtype='<U1')\n"
     ]
    }
   ],
   "source": [
    "embedding_weights = np.zeros((max_features, embeddings_dim))\n",
    "\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if index < max_features:\n",
    "        try: \n",
    "            embedding_weights[index,:] = embeddings[word]\n",
    "        except: \n",
    "            embedding_weights[index,:] = np.random.rand( 1 , embeddings_dim )\n",
    "\n",
    "le = preprocessing.LabelEncoder( )\n",
    "le.fit(train_labels + test_labels)\n",
    "train_labels = le.transform( train_labels )\n",
    "test_labels = le.transform( test_labels )\n",
    "print(\"Classes that are considered in the problem : \" + repr( le.classes_ ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes with bag-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.79919678714859432\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.80      0.80       124\n",
      "          1       0.80      0.80      0.80       125\n",
      "\n",
      "avg / total       0.80      0.80      0.80       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB( )\n",
    "model.fit( train_matrix , train_labels )\n",
    "results = model.predict( test_matrix )\n",
    "print(\"Accuracy = \" + repr( sklearn.metrics.accuracy_score(test_labels,results)))\n",
    "print()\n",
    "print(sklearn.metrics.classification_report( test_labels , results ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with bag-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.86      0.83       124\n",
      "          1       0.85      0.78      0.82       125\n",
      "\n",
      "avg / total       0.83      0.82      0.82       249\n",
      "\n",
      "Accuracy = 0.82329317269076308\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC( random_state=0 )\n",
    "model.fit( train_matrix , train_labels )\n",
    "results = model.predict( test_matrix )\n",
    "print(sklearn.metrics.classification_report(test_labels,results))\n",
    "print(\"Accuracy = \" + repr(sklearn.metrics.accuracy_score(test_labels,results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB-SVM with bag-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.86      0.83       124\n",
      "          1       0.85      0.78      0.82       125\n",
      "\n",
      "avg / total       0.83      0.82      0.82       249\n",
      "\n",
      "Accuracy = 0.82329317269076308\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB(fit_prior=False)\n",
    "model.fit( train_matrix,train_labels)\n",
    "train_matrix = np.hstack((train_matrix, model.predict_proba(train_matrix)))\n",
    "test_matrix = np.hstack((test_matrix, model.predict_proba(test_matrix)))\n",
    "model = LinearSVC( random_state=0 )\n",
    "model.fit( train_matrix , train_labels )\n",
    "results = model.predict( test_matrix )\n",
    "train_matrix = train_matrix[0: train_matrix.shape[0], 0: train_matrix.shape[1] - model.intercept_.shape[0] ]\n",
    "test_matrix = test_matrix[0: train_matrix.shape[0], 0: test_matrix.shape[1] - model.intercept_.shape[0] ]\n",
    "print(sklearn.metrics.classification_report(test_labels,results))\n",
    "print(\"Accuracy = \" + repr(sklearn.metrics.accuracy_score(test_labels,results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Method = KNN with word mover's distance as described in 'From Word Embeddings To Document Distances'\")\n",
    "model = WordMoversKNN(W_embed=embedding_weights , n_neighbors=3)\n",
    "model.fit( train_matrix , train_labels )\n",
    "results = model.predict( test_matrix )\n",
    "print(sklearn.metrics.classification_report(test_labels,results))\n",
    "print(\"Accuracy = \" + repr(sklearn.metrics.accuracy_score(test_labels,results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP with bag-of-words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "model.add(Dense(embeddings_dim, input_dim=train_matrix.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(embeddings_dim, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "if num_classes == 2: \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')\n",
    "else: \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 150 samples\n",
      "Epoch 1/10\n",
      "600/600 [==============================] - 2s - loss: 1.3669e-04 - val_loss: 4.1401e-04\n",
      "Epoch 2/10\n",
      "600/600 [==============================] - 2s - loss: 1.3759e-04 - val_loss: 4.0577e-04\n",
      "Epoch 3/10\n",
      "600/600 [==============================] - 2s - loss: 7.8077e-05 - val_loss: 4.0025e-04\n",
      "Epoch 4/10\n",
      "600/600 [==============================] - 2s - loss: 7.2978e-05 - val_loss: 3.9636e-04\n",
      "Epoch 5/10\n",
      "600/600 [==============================] - 2s - loss: 8.5598e-05 - val_loss: 3.9643e-04\n",
      "Epoch 6/10\n",
      "600/600 [==============================] - 1s - loss: 8.5797e-05 - val_loss: 3.9865e-04\n",
      "Epoch 7/10\n",
      "600/600 [==============================] - 2s - loss: 6.1198e-05 - val_loss: 3.9838e-04\n",
      "Epoch 8/10\n",
      "600/600 [==============================] - 2s - loss: 6.1486e-05 - val_loss: 3.9707e-04\n",
      "Epoch 9/10\n",
      "600/600 [==============================] - 2s - loss: 6.3637e-05 - val_loss: 3.9355e-04\n",
      "Epoch 10/10\n",
      "600/600 [==============================] - 2s - loss: 5.9621e-05 - val_loss: 3.9027e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=train_matrix, y=train_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/249 [======================>.......] - ETA: 0s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.86      0.83       124\n",
      "          1       0.85      0.78      0.82       125\n",
      "\n",
      "avg / total       0.83      0.82      0.82       249\n",
      "\n",
      "Accuracy = 0.82329317269076308\n"
     ]
    }
   ],
   "source": [
    "results = model.predict_classes(test_matrix)\n",
    "print()\n",
    "print(sklearn.metrics.classification_report(test_labels,results))\n",
    "print(\"Accuracy = \" + repr(sklearn.metrics.accuracy_score(test_labels,results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack of two LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dsbatista/virtual_envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", return_sequences=True, units=300, recurrent_activation=\"hard_sigmoid\")`\n",
      "  \n",
      "/Users/dsbatista/virtual_envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", units=300, recurrent_activation=\"hard_sigmoid\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/dsbatista/virtual_envs/python3/lib/python3.6/site-packages/keras/models.py:837: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n",
      "/Users/dsbatista/virtual_envs/python3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:2250: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "750/750 [==============================] - 14s - loss: 0.7124    \n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 14s - loss: 0.7004    \n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 15s - loss: 0.6953    \n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 15s - loss: 0.6323    \n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 15s - loss: 0.3603    \n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 16s - loss: 0.1820    \n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 17s - loss: 0.1240    \n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 17s - loss: 0.0516    \n",
      "Epoch 9/30\n",
      "750/750 [==============================] - 15s - loss: 0.0451    \n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 15s - loss: 0.0656    \n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 16s - loss: 0.0336    \n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 15s - loss: 0.0113    \n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 15s - loss: 0.0053    \n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 15s - loss: 0.0024    \n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 14s - loss: 0.0017    \n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 14s - loss: 0.0011    \n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 15s - loss: 0.0010    \n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 14s - loss: 9.9194e-04    \n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 14s - loss: 8.0742e-04    \n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 14s - loss: 7.0629e-04    \n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 14s - loss: 4.9305e-04    \n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 14s - loss: 7.3496e-04    \n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 14s - loss: 7.2544e-04    \n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 14s - loss: 0.0014    \n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 14s - loss: 3.3800e-04    \n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 14s - loss: 4.0153e-04    \n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 15s - loss: 3.5266e-04    \n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 14s - loss: 3.4496e-04    \n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 14s - loss: 0.0172    \n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 15s - loss: 0.0653    \n",
      "249/249 [==============================] - 1s     \n",
      "Accuracy = 0.82730923694779113\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.80      0.82       124\n",
      "          1       0.81      0.86      0.83       125\n",
      "\n",
      "avg / total       0.83      0.83      0.83       249\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embeddings_dim, input_length=max_sent_len, \n",
    "                    mask_zero=True, weights=[embedding_weights]))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(output_dim=embeddings_dim , \n",
    "               activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(output_dim=embeddings_dim , activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "if num_classes == 2: \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')\n",
    "else: \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')  \n",
    "\n",
    "model.fit( train_sequences , train_labels , epochs=30, batch_size=32)\n",
    "results = model.predict_classes( test_sequences )\n",
    "print (\"Accuracy = \" + repr( sklearn.metrics.accuracy_score( test_labels , results )  ))\n",
    "print (sklearn.metrics.classification_report( test_labels , results ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN from the paper: _Convolutional Neural Networks for Sentence Classification_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-e2314b095fd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnb_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sent_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_sent_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_weights\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'embedding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Graph' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "nb_filter = embeddings_dim\n",
    "\n",
    "model = Graph()\n",
    "\n",
    "model.add_input(name='input', input_shape=(max_sent_len,), dtype=int)\n",
    "\n",
    "model.add_node(Embedding(max_features, embeddings_dim, input_length=max_sent_len, mask_zero=False, \n",
    "                         weights=[embedding_weights] ), name='embedding', input='input')\n",
    "\n",
    "model.add_node(Dropout(0.25), name='dropout_embedding', input='embedding')\n",
    "\n",
    "\n",
    "for n_gram in [3, 5, 7]:\n",
    "    model.add_node(Convolution1D(nb_filter=nb_filter, filter_length=n_gram, border_mode='valid', activation='relu', subsample_length=1, input_dim=embeddings_dim, input_length=max_sent_len), name='conv_' + str(n_gram), input='dropout_embedding')\n",
    "    model.add_node(MaxPooling1D(pool_length=max_sent_len - n_gram + 1), name='maxpool_' + str(n_gram), input='conv_' + str(n_gram))\n",
    "    model.add_node(Flatten(), name='flat_' + str(n_gram), input='maxpool_' + str(n_gram))\n",
    "model.add_node(Dropout(0.25), name='dropout', inputs=['flat_' + str(n) for n in [3, 5, 7]])\n",
    "model.add_node(Dense(1, input_dim=nb_filter * len([3, 5, 7])), name='dense', input='dropout')\n",
    "model.add_node(Activation('sigmoid'), name='sigmoid', input='dense')\n",
    "model.add_output(name='output', input='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if num_classes == 2:\n",
    "    model.compile(loss={'output': 'binary_crossentropy'}, optimizer='adam')\n",
    "else: \n",
    "    model.compile(loss={'output': 'categorical_crossentropy'}, optimizer='adam') \n",
    "\n",
    "model.fit({'input': train_sequences, 'output': train_labels}, batch_size=32, nb_epoch=30)\n",
    "results = np.array(model.predict({'input': test_sequences}, batch_size=32)['output'])\n",
    "\n",
    "if num_classes != 2: \n",
    "    results = results.argmax(axis=-1)\n",
    "else: \n",
    "    results = (results > 0.5).astype('int32')\n",
    "\n",
    "print (sklearn.metrics.classification_report( test_labels , results ))\n",
    "print (\"Accuracy = \" + repr( sklearn.metrics.accuracy_score( test_labels , results )  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Method = Bidirectional LSTM\")\n",
    "np.random.seed(0)\n",
    "model = Graph()\n",
    "model.add_input(name='input', input_shape=(max_sent_len,), dtype=int)\n",
    "model.add_node(Embedding( max_features, embeddings_dim, input_length=max_sent_len, mask_zero=True, weights=[embedding_weights] ), name='embedding', input='input')\n",
    "model.add_node(LSTM(embeddings_dim, activation='sigmoid', inner_activation='hard_sigmoid', return_sequences=True), name='forward1', input='embedding')\n",
    "model.add_node(Dropout(0.25), name=\"dropout1\", input='forward1')\n",
    "model.add_node(LSTM(embeddings_dim, activation='sigmoid', inner_activation='hard_sigmoid'), name='forward2', input='forward1')\n",
    "model.add_node(LSTM(embeddings_dim, activation='sigmoid', inner_activation='hard_sigmoid', go_backwards=True, return_sequences=True), name='backward1', input='embedding')\n",
    "model.add_node(Dropout(0.25), name=\"dropout2\", input='backward1') \n",
    "model.add_node(LSTM(embeddings_dim, activation='sigmoid', inner_activation='hard_sigmoid', go_backwards=True), name='backward2', input='backward1')\n",
    "model.add_node(Dropout(0.25), name='dropout', inputs=['forward2', 'backward2'])\n",
    "model.add_node(Dense(1, activation='sigmoid'), name='sigmoid', input='dropout')\n",
    "model.add_output(name='output', input='sigmoid')\n",
    "if num_classes == 2: model.compile(loss={'output': 'binary_crossentropy'}, optimizer='adam')\n",
    "else: model.compile(loss={'output': 'categorical_crossentropy'}, optimizer='adam')\n",
    "model.fit({'input': train_sequences, 'output': train_labels}, batch_size=32, nb_epoch=30)\n",
    "results = np.array(model.predict({'input': test_sequences}, batch_size=32)['output'])\n",
    "if num_classes != 2: results = results.argmax(axis=-1)\n",
    "else: results = (results > 0.5).astype('int32')\n",
    "print (\"Accuracy = \" + repr( sklearn.metrics.accuracy_score( test_labels , results )  ))\n",
    "print (sklearn.metrics.classification_report( test_labels , results ))\n",
    "\n",
    "print (\"Method = CNN-LSTM\")\n",
    "np.random.seed(0)\n",
    "filter_length = 3\n",
    "nb_filter = embeddings_dim\n",
    "pool_length = 2\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embeddings_dim, input_length=max_sent_len, weights=[embedding_weights]))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution1D(nb_filter=nb_filter, filter_length=filter_length, border_mode='valid', activation='relu', subsample_length=1))\n",
    "model.add(MaxPooling1D(pool_length=pool_length))\n",
    "model.add(LSTM(embeddings_dim))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "if num_classes == 2: model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')\n",
    "else: model.compile(loss='categorical_crossentropy', optimizer='adam')  \n",
    "model.fit( train_sequences , train_labels , nb_epoch=30, batch_size=32)\n",
    "results = model.predict_classes( test_sequences )\n",
    "print (\"Accuracy = \" + repr( sklearn.metrics.accuracy_score( test_labels , results ) ) )\n",
    "print (sklearn.metrics.classification_report( test_labels , results ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Method = Linear SVM with doc2vec features\")\n",
    "np.random.seed(0)\n",
    "class LabeledLineSentence(object):\n",
    "  def __init__(self, data ): self.data = data\n",
    "  def __iter__(self):\n",
    "    for uid, line in enumerate( self.data ): yield TaggedDocument( line.split(\" \") , [\"S_%s\" % uid] )\n",
    "model = Doc2Vec( alpha=0.025 , min_alpha=0.025 )\n",
    "sentences = LabeledLineSentence( train_texts + test_texts )\n",
    "model.build_vocab( sentences )\n",
    "model.train( sentences )\n",
    "for w in model.vocab.keys():\n",
    "  try: model[w] = embeddings[w] \n",
    "  except : continue\n",
    "for epoch in range(10):\n",
    "    model.train(sentences)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "train_rep = np.array( [ model.docvecs[i] for i in range( train_matrix.shape[0] ) ] )\n",
    "test_rep = np.array( [ model.docvecs[i + train_matrix.shape[0]] for i in range( test_matrix.shape[0] ) ] )\n",
    "model = LinearSVC( random_state=0 )\n",
    "model.fit( train_rep , train_labels )\n",
    "results = model.predict( test_rep )\n",
    "print (\"Accuracy = \" + repr( sklearn.metrics.accuracy_score( test_labels , results )  ))\n",
    "print (sklearn.metrics.classification_report( test_labels , results ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Method = Non-linear SVM with doc2vec features\")\n",
    "np.random.seed(0)\n",
    "class LabeledLineSentence(object):\n",
    "  def __init__(self, data ): self.data = data\n",
    "  def __iter__(self):\n",
    "    for uid, line in enumerate( self.data ): yield TaggedDocument( line.split(\" \") , [\"S_%s\" % uid] )\n",
    "model = Doc2Vec( alpha=0.025 , min_alpha=0.025 )\n",
    "sentences = LabeledLineSentence( train_texts + test_texts )\n",
    "model.build_vocab( sentences )\n",
    "model.train( sentences )\n",
    "for w in model.vocab.keys():\n",
    "  try: model[w] = embeddings[w] \n",
    "  except : continue\n",
    "for epoch in range(10):\n",
    "    model.train(sentences)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "train_rep = np.array( [ model.docvecs[i] for i in range( train_matrix.shape[0] ) ] )\n",
    "test_rep = np.array( [ model.docvecs[i + train_matrix.shape[0]] for i in range( test_matrix.shape[0] ) ] )\n",
    "model = SVC( random_state=0 , kernel='poly' )\n",
    "model.fit( train_rep , train_labels )\n",
    "results = model.predict( test_rep )\n",
    "print (\"Accuracy = \" + repr( sklearn.metrics.accuracy_score( test_labels , results )  ))\n",
    "print (sklearn.metrics.classification_report( test_labels , results ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Method = MLP with doc2vec features\")\n",
    "np.random.seed(0)\n",
    "class LabeledLineSentence(object):\n",
    "  def __init__(self, data ): self.data = data\n",
    "  def __iter__(self):\n",
    "    for uid, line in enumerate( self.data ): yield TaggedDocument( line.split(\" \") , [\"S_%s\" % uid] )\n",
    "model = Doc2Vec( alpha=0.025 , min_alpha=0.025 )\n",
    "sentences = train_texts + test_texts\n",
    "sentences = LabeledLineSentence( sentences )\n",
    "model.build_vocab( sentences )\n",
    "model.train( sentences )\n",
    "for w in model.vocab.keys():\n",
    "  try: model[w] = embeddings[w]\n",
    "  except : continue\n",
    "for epoch in range(10):\n",
    "    model.train(sentences)\n",
    "    model.alpha -= 0.002\n",
    "    model.min_alpha = model.alpha\n",
    "train_rep = np.array( [ model.docvecs[i] for i in range( train_matrix.shape[0] ) ] )\n",
    "test_rep = np.array( [ model.docvecs[i + train_matrix.shape[0]] for i in range( test_matrix.shape[0] ) ] )\n",
    "model = Sequential()\n",
    "model.add(Dense(embeddings_dim, input_dim=train_rep.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(embeddings_dim, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "if num_classes == 2: model.compile(loss='binary_crossentropy', optimizer='adam', class_mode='binary')\n",
    "else: model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit( train_rep , train_labels , nb_epoch=30, batch_size=32)\n",
    "results = model.predict_classes( test_rep )\n",
    "print (\"Accuracy = \" + repr( sklearn.metrics.accuracy_score( test_labels , results )  ))\n",
    "print (sklearn.metrics.classification_report( test_labels , results ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
